{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45533b32",
      "metadata": {
        "id": "45533b32"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install gym\n",
        "!pip install torch\n",
        "!pip install imageio\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import time\n",
        "import imageio\n",
        "import os\n",
        "from collections import deque\n",
        "from gym import Env, spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "# Maze Environment\n",
        "class MazeEnv(Env):\n",
        "    def __init__(self):\n",
        "        super(MazeEnv, self).__init__()\n",
        "        self.maze = np.array([\n",
        "            [2, 0, 0, 0, 0, 1],\n",
        "            [0, 0, 1, 1, 0, 1],\n",
        "            [0, 0, 1, 1, 0, 1],\n",
        "            [0, 0, 0, 1, 0, 0],\n",
        "            [0, 0, 0, 0, 1, 0],\n",
        "            [1, 1, 0, 1, 0, 3]\n",
        "        ])\n",
        "        self.start_pos = (0, 0)\n",
        "        self.end_pos = (5, 5)\n",
        "        self.agent_pos = list(self.start_pos)\n",
        "        self.max_steps = 100\n",
        "        self.current_step = 0\n",
        "        self.action_space = spaces.Discrete(4)  # Up, Right, Down, Left\n",
        "        self.observation_space = spaces.Box(low=0, high=5, shape=(2,), dtype=np.int32)\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = list(self.start_pos)\n",
        "        self.current_step = 0\n",
        "        return np.array(self.agent_pos, dtype=np.int32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        row, col = self.agent_pos\n",
        "\n",
        "        # Define movement based on action\n",
        "        new_pos = {\n",
        "            0: [row-1, col],  # Up\n",
        "            1: [row, col+1],  # Right\n",
        "            2: [row+1, col],  # Down\n",
        "            3: [row, col-1]   # Left\n",
        "        }.get(action, [row, col])\n",
        "\n",
        "        # Check boundaries and walls\n",
        "        if (0 <= new_pos[0] < 6 and\n",
        "            0 <= new_pos[1] < 6 and\n",
        "            self.maze[tuple(new_pos)] != 1):\n",
        "            self.agent_pos = new_pos\n",
        "\n",
        "        done = (self.agent_pos == list(self.end_pos)) or (self.current_step >= self.max_steps)\n",
        "        reward = 50 if done and self.agent_pos == list(self.end_pos) else -0.1\n",
        "\n",
        "        return np.array(self.agent_pos, dtype=np.int32), reward, done, {}\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        fig, ax = plt.subplots(figsize=(5, 5))\n",
        "        ax.clear()\n",
        "\n",
        "        # Color grid: Yellow=empty, Black=wall, Green=start, Red=end, Blue=agent\n",
        "        grid = np.full((6, 6, 3), [255, 255, 0], dtype=np.uint8)  # Yellow\n",
        "        grid[self.maze == 1] = [0, 0, 0]      # Black\n",
        "        grid[self.maze == 2] = [0, 200, 0]    # Green\n",
        "        grid[self.maze == 3] = [255, 0, 0]    # Red\n",
        "        grid[tuple(self.agent_pos)] = [0, 0, 255]  # Blue\n",
        "\n",
        "        ax.imshow(grid)\n",
        "        ax.set_xticks([])  # Hide x axis labels\n",
        "        ax.set_yticks([])  # Hide y axis labels\n",
        "        fig.tight_layout(pad=0)\n",
        "\n",
        "        fig.canvas.draw()\n",
        "        img = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
        "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
        "        plt.close(fig)\n",
        "        return img[..., :3]\n",
        "\n",
        "\n",
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_size=2, action_size=4, device=None):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.05\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.0001\n",
        "        self.batch_size = 128\n",
        "        self.tau = 0.01\n",
        "\n",
        "        self.model = self._build_model().to(self.device)\n",
        "        self.target_model = self._build_model().to(self.device)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = nn.Sequential(\n",
        "            nn.Linear(self.state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, self.action_size)\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "\n",
        "        state_normalized = state / 5.0  # Normalize the state\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            return torch.argmax(self.model(state_tensor)).item()\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = torch.tensor(np.array([t[0] for t in minibatch]), dtype=torch.float32).to(self.device)\n",
        "        actions = torch.tensor([t[1] for t in minibatch], dtype=torch.long).to(self.device)\n",
        "        rewards = torch.tensor([t[2] for t in minibatch], dtype=torch.float32).to(self.device)\n",
        "        next_states = torch.tensor(np.array([t[3] for t in minibatch]), dtype=torch.float32).to(self.device)\n",
        "        dones = torch.tensor([t[4] for t in minibatch], dtype=torch.float32).to(self.device)\n",
        "\n",
        "        targets = self.model(states)\n",
        "        next_q = self.target_model(next_states)\n",
        "\n",
        "        batch_index = torch.arange(self.batch_size).to(self.device)\n",
        "        targets[batch_index, actions] = rewards + self.gamma * next_q.max(dim=1)[0] * (1 - dones)\n",
        "\n",
        "        loss = nn.MSELoss()(targets, self.model(states))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        self.soft_update()\n",
        "\n",
        "    def soft_update(self):\n",
        "        \"\"\"Update target model with soft update.\"\"\"\n",
        "        for target_param, param in zip(self.target_model.parameters(), self.model.parameters()):\n",
        "            target_param.data.copy_((1.0 - self.tau) * target_param.data + self.tau * param.data)\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.model.state_dict(), path + \".weights.pth\")\n",
        "\n",
        "    def load(self, path):\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "        self.update_target_model()\n",
        "\n",
        "\n",
        "# Dyna-Q Agent\n",
        "class DynaQAgent:\n",
        "    def __init__(self, num_states=36, num_actions=4, device=None):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.q_table = torch.zeros((num_states, num_actions), device=self.device)\n",
        "        self.model = {}\n",
        "        self.alpha = 0.1\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.9995\n",
        "        self.planning_steps = 10\n",
        "\n",
        "    def state_to_index(self, state):\n",
        "        return state[0] * 6 + state[1]\n",
        "\n",
        "    def act(self, state):\n",
        "        state_idx = self.state_to_index(state)\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.randrange(self.num_actions)\n",
        "        return torch.argmax(self.q_table[state_idx]).item()\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        state_idx = self.state_to_index(state)\n",
        "        next_state_idx = self.state_to_index(next_state)\n",
        "\n",
        "        best_next = self.q_table[next_state_idx].max() if not done else 0\n",
        "        td_target = reward + self.gamma * best_next\n",
        "        self.q_table[state_idx, action] += self.alpha * (td_target - self.q_table[state_idx, action])\n",
        "\n",
        "        self.model[(state_idx, action)] = (reward, next_state_idx)\n",
        "\n",
        "        # Planning through model\n",
        "        if len(self.model) > 0:\n",
        "            samples = random.choices(list(self.model.items()), k=self.planning_steps)\n",
        "            for (s, a), (r, ns) in samples:\n",
        "                best_next = self.q_table[ns].max()\n",
        "                self.q_table[s, a] += self.alpha * (r + self.gamma * best_next - self.q_table[s, a])\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.q_table.cpu(), path + \".qtable.pth\")\n",
        "\n",
        "    def load(self, path):\n",
        "        self.q_table = torch.load(path).to(self.device)\n",
        "\n",
        "\n",
        "# Training and Testing Functions\n",
        "def train(agent_type, num_episodes, model_path, video_dir, device):\n",
        "    env = MazeEnv()\n",
        "    if agent_type == 'dqn':\n",
        "        agent = DQNAgent(device=device)\n",
        "    elif agent_type == 'dynaq':\n",
        "        agent = DynaQAgent(device=device)\n",
        "\n",
        "    os.makedirs(video_dir, exist_ok=True)\n",
        "\n",
        "    rewards = []\n",
        "    avg_rewards = []\n",
        "    epsilons = []\n",
        "    num_steps = []  # To track the number of steps\n",
        "    start_time = time.time()\n",
        "    best_avg_reward = -float('inf')\n",
        "\n",
        "    print(f\"Starting training for {num_episodes} episodes...\")\n",
        "\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        done = False\n",
        "        episode_frames = []\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            shaped_reward = reward + (1 - np.sqrt((5 - next_state[0])**2 + (5 - next_state[1])**2) / 14.14) * 0.5\n",
        "\n",
        "            if agent_type == 'dqn':\n",
        "                agent.remember(state, action, shaped_reward, next_state, done)\n",
        "                agent.replay()\n",
        "            elif agent_type == 'dynaq':\n",
        "                agent.update(state, action, shaped_reward, next_state, done)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += shaped_reward\n",
        "            steps += 1\n",
        "\n",
        "            if episode % 10 == 0 and len(episode_frames) < 100:\n",
        "                frame = env.render()\n",
        "                if frame is not None:\n",
        "                    episode_frames.append(np.clip(frame, 0, 255).astype(np.uint8))\n",
        "\n",
        "        num_steps.append(steps)\n",
        "\n",
        "        if episode_frames:\n",
        "            gif_path = os.path.join(video_dir, f'episode_{episode+1:04d}.gif')\n",
        "            imageio.mimsave(gif_path, episode_frames, fps=5)\n",
        "\n",
        "        rewards.append(total_reward)\n",
        "        avg_rewards.append(np.mean(rewards[-100:]) if rewards else 0)\n",
        "        epsilons.append(agent.epsilon)\n",
        "\n",
        "        print(f\"Episode {episode + 1} | Total Reward: {total_reward:.2f} | Avg Reward (Last 100): {avg_rewards[-1]:.2f} | Steps: {steps} | Epsilon: {agent.epsilon:.3f}\")\n",
        "\n",
        "        if avg_rewards[-1] > best_avg_reward:\n",
        "            agent.save(f\"{model_path}_best\")\n",
        "            best_avg_reward = avg_rewards[-1]\n",
        "\n",
        "    agent.save(f\"{model_path}_best\")\n",
        "    print(f\"Training completed! Best average reward: {best_avg_reward:.2f}\")\n",
        "\n",
        "    # Plotting the training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(rewards, label=\"Total Reward\")\n",
        "    plt.plot(avg_rewards, label=\"Avg Reward (Last 100)\")\n",
        "    plt.title(\"Training Rewards\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(num_steps, label=\"Steps per Episode\")\n",
        "    plt.title(\"Steps per Episode\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Steps\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Testing the Agent\n",
        "def test(agent_type, model_path, video_dir, device):\n",
        "    env = MazeEnv()\n",
        "    if agent_type == 'dqn':\n",
        "        agent = DQNAgent(device=device)\n",
        "    elif agent_type == 'dynaq':\n",
        "        agent = DynaQAgent(device=device)\n",
        "\n",
        "    agent.load(f\"{model_path}_best.weights.pth\" if agent_type == 'dqn' else f\"{model_path}_best.qtable.pth\")\n",
        "    agent.epsilon = 0.0  # No exploration\n",
        "\n",
        "    os.makedirs(video_dir, exist_ok=True)\n",
        "    successful_episodes = 0\n",
        "    total_steps = []  # To track the number of steps in test episodes\n",
        "\n",
        "    for episode in range(10):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        frames = []\n",
        "        steps = 0\n",
        "\n",
        "        while not done and len(frames) < 100:\n",
        "            action = agent.act(state)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            frames.append(env.render())\n",
        "            steps += 1\n",
        "\n",
        "        gif_path = os.path.join(video_dir, f'test_ep_{episode+1}.gif')\n",
        "        imageio.mimsave(gif_path, frames, fps=5)\n",
        "\n",
        "        if done:\n",
        "            successful_episodes += 1\n",
        "        total_steps.append(steps)\n",
        "        print(f\"Test {episode+1}: {'Success' if done else 'Fail'} | Steps: {steps}\")\n",
        "\n",
        "    success_rate = successful_episodes / 10\n",
        "    avg_steps = np.mean(total_steps)\n",
        "    print(f\"\\nSuccess Rate: {success_rate:.0%} | Avg Steps per Episode: {avg_steps:.2f}\")\n",
        "\n",
        "# Running Example: Train and Test both DQN and Dyna-Q agents\n",
        "train('dqn', 1000, 'maze_agent_dqn', 'dqn_training_gifs', 'cpu')\n",
        "test('dqn', 'maze_agent_dqn', 'dqn_test_gifs', 'cpu')\n",
        "\n",
        "train('dynaq', 1000, 'maze_agent_dynaq', 'dynaq_training_gifs', 'cpu')\n",
        "test('dynaq', 'maze_agent_dynaq', 'dynaq_test_gifs', 'cpu')\n",
        "\n",
        "# Displaying gifs\n",
        "from IPython.display import display, Image\n",
        "\n",
        "# Displaying DQN test gifs\n",
        "print(\"Displaying DQN Test GIFs\")\n",
        "for i in range(1, 11):\n",
        "    display(Image(filename=f'dqn_test_gifs/test_ep_{i}.gif'))\n",
        "\n",
        "# Displaying Dyna-Q test gifs\n",
        "print(\"Displaying Dyna-Q Test GIFs\")\n",
        "for i in range(1, 11):\n",
        "    display(Image(filename=f'dynaq_test_gifs/test_ep_{i}.gif'))\n"
      ],
      "metadata": {
        "id": "wbb-5UUvDk1I"
      },
      "id": "wbb-5UUvDk1I",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}