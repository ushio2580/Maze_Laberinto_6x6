{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ushio2580/Maze_Laberinto_6x6/blob/main/Maze_No_Rand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Maze Environment\n",
        "\n",
        "The `MazeEnv` class defines a 6x6 grid maze where an agent navigates from a start position to a goal, avoiding walls.\n",
        "\n",
        "- **Maze Layout:**\n",
        "  - `0`: Empty cells (walkable)\n",
        "  - `1`: Walls (obstacles)\n",
        "  - `2`: Start position\n",
        "  - `3`: Goal position\n",
        "\n",
        "- **Key Methods:**\n",
        "  - `reset()`: Places the agent back at the start position and returns the initial state.\n",
        "  - `step(action)`: Executes the agent's action (up, right, down, left), returning the new state, reward, and whether the episode is done.\n",
        "  - `render()`: Creates an RGB image of the maze for visualization."
      ],
      "metadata": {
        "id": "3EM1CiozEP4L"
      },
      "id": "3EM1CiozEP4L"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!pip install torch\n",
        "!pip install imageio"
      ],
      "metadata": {
        "id": "wPfBtjkoEQ0g"
      },
      "id": "wPfBtjkoEQ0g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "IqtjCbCDF2M2"
      },
      "id": "IqtjCbCDF2M2"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import time\n",
        "import imageio\n",
        "import os\n",
        "from collections import deque\n",
        "from gym import Env, spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')"
      ],
      "metadata": {
        "id": "YR_gwKZJF0gy"
      },
      "id": "YR_gwKZJF0gy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Maze Environment**"
      ],
      "metadata": {
        "id": "1zKlHqMUF8XD"
      },
      "id": "1zKlHqMUF8XD"
    },
    {
      "cell_type": "code",
      "source": [
        "class MazeEnv(Env):\n",
        "    def __init__(self):\n",
        "        super(MazeEnv, self).__init__()\n",
        "        self.maze = np.array([\n",
        "            [2, 0, 0, 0, 0, 1],\n",
        "            [0, 0, 1, 1, 0, 1],\n",
        "            [0, 0, 1, 1, 0, 1],\n",
        "            [0, 0, 0, 1, 0, 0],\n",
        "            [0, 0, 0, 0, 1, 0],\n",
        "            [1, 1, 0, 1, 0, 3]\n",
        "        ])\n",
        "        self.start_pos = (0, 0)\n",
        "        self.end_pos = (5, 5)\n",
        "        self.agent_pos = list(self.start_pos)\n",
        "        self.max_steps = 100\n",
        "        self.current_step = 0\n",
        "        self.action_space = spaces.Discrete(4)  # Up, Right, Down, Left\n",
        "        self.observation_space = spaces.Box(low=0, high=5, shape=(2,), dtype=np.int32)\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_pos = list(self.start_pos)\n",
        "        self.current_step = 0\n",
        "        return np.array(self.agent_pos, dtype=np.int32)\n",
        "\n",
        "    def step(self, action):\n",
        "        self.current_step += 1\n",
        "        row, col = self.agent_pos\n",
        "        new_pos = {\n",
        "            0: [row-1, col],  # Up\n",
        "            1: [row, col+1],  # Right\n",
        "            2: [row+1, col],  # Down\n",
        "            3: [row, col-1]   # Left\n",
        "        }.get(action, [row, col])\n",
        "        if (0 <= new_pos[0] < 6 and\n",
        "            0 <= new_pos[1] < 6 and\n",
        "            self.maze[tuple(new_pos)] != 1):\n",
        "            self.agent_pos = new_pos\n",
        "        done = (self.agent_pos == list(self.end_pos)) or (self.current_step >= self.max_steps)\n",
        "        reward = 50 if done and self.agent_pos == list(self.end_pos) else -0.1\n",
        "        return np.array(self.agent_pos, dtype=np.int32), reward, done, {}\n",
        "\n",
        "    def render(self, mode='rgb_array'):\n",
        "        fig, ax = plt.subplots(figsize=(5, 5))\n",
        "        ax.clear()\n",
        "        grid = np.full((6, 6, 3), [255, 255, 0], dtype=np.uint8)  # Yellow\n",
        "        grid[self.maze == 1] = [0, 0, 0]      # Black\n",
        "        grid[self.maze == 2] = [0, 200, 0]    # Green\n",
        "        grid[self.maze == 3] = [255, 0, 0]    # Red\n",
        "        grid[tuple(self.agent_pos)] = [0, 0, 255]  # Blue\n",
        "        ax.imshow(grid)\n",
        "        ax.set_xticks([])\n",
        "        ax.set_yticks([])\n",
        "        fig.tight_layout(pad=0)\n",
        "        fig.canvas.draw()\n",
        "        img = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
        "        img = img.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
        "        plt.close(fig)\n",
        "        return img[..., :3]"
      ],
      "metadata": {
        "id": "n3Vn1gvYF6Y-"
      },
      "id": "n3Vn1gvYF6Y-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define DQN Agent"
      ],
      "metadata": {
        "id": "TeAKSm_SGMlt"
      },
      "id": "TeAKSm_SGMlt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Q-Network (DQN)\n",
        "\n",
        "DQN uses a neural network to approximate the Q-function, which estimates the expected future rewards for taking action \\(a\\) in state \\(s\\). The Q-value update follows the Bellman equation:\n",
        "\n",
        " $$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right) $$\n",
        "\n",
        "However, DQN trains the network by minimizing a loss function:\n",
        "\n",
        " $$\\text{Loss} = \\mathbb{E} \\left[ \\left( r + \\gamma \\max_{a'} Q(s', a'; \\theta^-) - Q(s, a; \\theta) \\right)^2 \\right] $$\n",
        "\n",
        "- \\$( r \\$): Reward received\n",
        "- \\$( \\gamma \\$): Discount factor (e.g., 0.99)\n",
        "- \\$( \\theta \\$): Parameters of the current Q-network\n",
        "- \\$( \\theta^- \\$): Parameters of the target network (updated periodically)"
      ],
      "metadata": {
        "id": "bMs-CknJG_bS"
      },
      "id": "bMs-CknJG_bS"
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size=2, action_size=4, device=None):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.05\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.learning_rate = 0.0001\n",
        "        self.batch_size = 128\n",
        "        self.tau = 0.01\n",
        "        self.model = self._build_model().to(self.device)\n",
        "        self.target_model = self._build_model().to(self.device)\n",
        "        self.update_target_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = nn.Sequential(\n",
        "            nn.Linear(self.state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, self.action_size)\n",
        "        )\n",
        "        return model\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            return torch.argmax(self.model(state_tensor)).item()\n",
        "\n",
        "    def replay(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return\n",
        "        minibatch = random.sample(self.memory, self.batch_size)\n",
        "        states = torch.tensor(np.array([t[0] for t in minibatch]), dtype=torch.float32).to(self.device)\n",
        "        actions = torch.tensor([t[1] for t in minibatch], dtype=torch.long).to(self.device)\n",
        "        rewards = torch.tensor([t[2] for t in minibatch], dtype=torch.float32).to(self.device)\n",
        "        next_states = torch.tensor(np.array([t[3] for t in minibatch]), dtype=torch.float32).to(self.device)\n",
        "        dones = torch.tensor([t[4] for t in minibatch], dtype=torch.float32).to(self.device)\n",
        "        targets = self.model(states)\n",
        "        next_q = self.target_model(next_states)\n",
        "        batch_index = torch.arange(self.batch_size).to(self.device)\n",
        "        targets[batch_index, actions] = rewards + self.gamma * next_q.max(dim=1)[0] * (1 - dones)\n",
        "        loss = nn.MSELoss()(targets, self.model(states))\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "        self.soft_update()\n",
        "\n",
        "    def soft_update(self):\n",
        "        for target_param, param in zip(self.target_model.parameters(), self.model.parameters()):\n",
        "            target_param.data.copy_((1.0 - self.tau) * target_param.data + self.tau * param.data)\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.model.state_dict(), path + \".weights.pth\")\n",
        "\n",
        "    def load(self, path):\n",
        "        self.model.load_state_dict(torch.load(path))\n",
        "        self.update_target_model()"
      ],
      "metadata": {
        "id": "X2LNWm2nGN-p"
      },
      "id": "X2LNWm2nGN-p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Dyna-Q Agent"
      ],
      "metadata": {
        "id": "PpKNvS5HGVih"
      },
      "id": "PpKNvS5HGVih"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dyna-Q\n",
        "\n",
        "Dyna-Q combines Q-learning with a model-based planning approach. The Q-value update is identical to Q-learning:\n",
        "\n",
        "$$[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right) ]$$\n",
        "\n",
        "Additionally, Dyna-Q builds a model of the environment:\n",
        "- **Model Update:** Stores transitions as $$( \\text{model}(s, a) = (r, s') )$$.\n",
        "- **Planning:** Samples past experiences from the model to perform extra Q-value updates, improving sample efficiency."
      ],
      "metadata": {
        "id": "2OBc3DodHW-i"
      },
      "id": "2OBc3DodHW-i"
    },
    {
      "cell_type": "code",
      "source": [
        "class DynaQAgent:\n",
        "    def __init__(self, num_states=36, num_actions=4, device=None):\n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.qiculares = torch.zeros((num_states, num_actions), device=self.device)\n",
        "        self.model = {}\n",
        "        self.alpha = 0.1\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.9995\n",
        "        self.planning_steps = 10\n",
        "\n",
        "    def state_to_index(self, state):\n",
        "        return state[0] * 6 + state[1]\n",
        "\n",
        "    def act(self, state):\n",
        "        state_idx = self.state_to_index(state)\n",
        "        if random.uniform(0, 1) < self.epsilon:\n",
        "            return random.randrange(self.num_actions)\n",
        "        return torch.argmax(self.q_table[state_idx]).item()\n",
        "\n",
        "    def update(self, state, action, reward, next_state, done):\n",
        "        state_idx = self.state_to_index(state)\n",
        "        next_state_idx = self.state_to_index(next_state)\n",
        "        best_next = self.q_table[next_state_idx].max() if not done else 0\n",
        "        td_target = reward + self.gamma * best_next\n",
        "        self.q_table[state_idx, action] += self.alpha * (td_target - self.q_table[state_idx, action])\n",
        "        self.model[(state_idx, action)] = (reward, next_state_idx)\n",
        "        if len(self.model) > 0:\n",
        "            samples = random.choices(list(self.model.items()), k=self.planning_steps)\n",
        "            for (s, a), (r, ns) in samples:\n",
        "                best_next = self.q_table[ns].max()\n",
        "                self.q_table[s, a] += self.alpha * (r + self.gamma * best_next - self.q_table[s, a])\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def save(self, path):\n",
        "        torch.save(self.q_table.cpu(), path + \".qtable.pth\")\n",
        "\n",
        "    def load(self, path):\n",
        "        self.q_table = torch.load(path).to(self.device)"
      ],
      "metadata": {
        "id": "xJrGqRi3GRcm"
      },
      "id": "xJrGqRi3GRcm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Function"
      ],
      "metadata": {
        "id": "kY_B_JnlGZjS"
      },
      "id": "kY_B_JnlGZjS"
    },
    {
      "cell_type": "code",
      "source": [
        "def train(agent_type, num_episodes, model_path, video_dir, device):\n",
        "    env = MazeEnv()\n",
        "    if agent_type == 'dqn':\n",
        "        agent = DQNAgent(device=device)\n",
        "    elif agent_type == 'dynaq':\n",
        "        agent = DynaQAgent(device=device)\n",
        "    os.makedirs(video_dir, exist_ok=True)\n",
        "    rewards = []\n",
        "    avg_rewards = []\n",
        "    epsilons = []\n",
        "    num_steps = []\n",
        "    start_time = time.time()\n",
        "    best_avg_reward = -float('inf')\n",
        "    print(f\"Starting training for {num_episodes} episodes...\")\n",
        "    for episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        steps = 0\n",
        "        done = False\n",
        "        episode_frames = []\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            shaped_reward = reward + (1 - np.sqrt((5 - next_state[0])**2 + (5 - next_state[1])**2) / 14.14) * 0.5\n",
        "            if agent_type == 'dqn':\n",
        "                agent.remember(state, action, shaped_reward, next_state, done)\n",
        "                agent.replay()\n",
        "            elif agent_type == 'dynaq':\n",
        "                agent.update(state, action, shaped_reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += shaped_reward\n",
        "            steps += 1\n",
        "            if episode % 10 == 0 and len(episode_frames) < 100:\n",
        "                frame = env.render()\n",
        "                if frame is not None:\n",
        "                    episode_frames.append(np.clip(frame, 0, 255).astype(np.uint8))\n",
        "        num_steps.append(steps)\n",
        "        if episode_frames:\n",
        "            gif_path = os.path.join(video_dir, f'episode_{episode+1:04d}.gif')\n",
        "            imageio.mimsave(gif_path, episode_frames, fps=5)\n",
        "        rewards.append(total_reward)\n",
        "        avg_rewards.append(np.mean(rewards[-100:]) if rewards else 0)\n",
        "        epsilons.append(agent.epsilon)\n",
        "        print(f\"Episode {episode + 1} | Total Reward: {total_reward:.2f} | Avg Reward (Last 100): {avg_rewards[-1]:.2f} | Steps: {steps} | Epsilon: {agent.epsilon:.3f}\")\n",
        "        if avg_rewards[-1] > best_avg_reward:\n",
        "            agent.save(f\"{model_path}_best\")\n",
        "            best_avg_reward = avg_rewards[-1]\n",
        "    agent.save(f\"{model_path}_best\")\n",
        "    print(f\"Training completed! Best average reward: {best_avg_reward:.2f}\")\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(rewards, label=\"Total Reward\")\n",
        "    plt.plot(avg_rewards, label=\"Avg Reward (Last 100)\")\n",
        "    plt.title(\"Training Rewards\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(num_steps, label=\"Steps per Episode\")\n",
        "    plt.title(\"Steps per Episode\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Steps\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LoKmiudaGbO_"
      },
      "id": "LoKmiudaGbO_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Function"
      ],
      "metadata": {
        "id": "j1R2uet5Geyz"
      },
      "id": "j1R2uet5Geyz"
    },
    {
      "cell_type": "code",
      "source": [
        "def test(agent_type, model_path, video_dir, device):\n",
        "    env = MazeEnv()\n",
        "    if agent_type == 'dqn':\n",
        "        agent = DQNAgent(device=device)\n",
        "    elif agent_type == 'dynaq':\n",
        "        agent = DynaQAgent(device=device)\n",
        "    agent.load(f\"{model_path}_best.weights.pth\" if agent_type == 'dqn' else f\"{model_path}_best.qtable.pth\")\n",
        "    agent.epsilon = 0.0\n",
        "    os.makedirs(video_dir, exist_ok=True)\n",
        "    successful_episodes = 0\n",
        "    total_steps = []\n",
        "    for episode in range(10):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        frames = []\n",
        "        steps = 0\n",
        "        while not done and len(frames) < 100:\n",
        "            action = agent.act(state)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            frames.append(env.render())\n",
        "            steps += 1\n",
        "        gif_path = os.path.join(video_dir, f'test_ep_{episode+1}.gif')\n",
        "        imageio.mimsave(gif_path, frames, fps=5)\n",
        "        if done:\n",
        "            successful_episodes += 1\n",
        "        total_steps.append(steps)\n",
        "        print(f\"Test {episode+1}: {'Success' if done else 'Fail'} | Steps: {steps}\")\n",
        "    success_rate = successful_episodes / 10\n",
        "    avg_steps = np.mean(total_steps)\n",
        "    print(f\"\\nSuccess Rate: {success_rate:.0%} | Avg Steps per Episode: {avg_steps:.2f}\")"
      ],
      "metadata": {
        "id": "dPnLpk9oGgNS"
      },
      "id": "dPnLpk9oGgNS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Training and Testing (DQN)"
      ],
      "metadata": {
        "id": "4DZ5OS0jGjMT"
      },
      "id": "4DZ5OS0jGjMT"
    },
    {
      "cell_type": "code",
      "source": [
        "train('dqn', 1000, 'maze_agent_dqn', 'dqn_training_gifs', 'cpu')\n",
        "test('dqn', 'maze_agent_dqn', 'dqn_test_gifs', 'cpu')"
      ],
      "metadata": {
        "id": "YDo0qL0UGjnQ"
      },
      "id": "YDo0qL0UGjnQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Training and Testing (Dyna-Q)"
      ],
      "metadata": {
        "id": "cmaHqqOaGrpp"
      },
      "id": "cmaHqqOaGrpp"
    },
    {
      "cell_type": "code",
      "source": [
        "train('dynaq', 1000, 'maze_agent_dynaq', 'dynaq_training_gifs', 'cpu')\n",
        "test('dynaq', 'maze_agent_dynaq', 'dynaq_test_gifs', 'cpu')"
      ],
      "metadata": {
        "id": "OQ9aaoiZGsCy"
      },
      "id": "OQ9aaoiZGsCy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display GIFs\n"
      ],
      "metadata": {
        "id": "IIa8lcwoGvus"
      },
      "id": "IIa8lcwoGvus"
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Image\n",
        "print(\"Displaying DQN Test GIFs\")\n",
        "for i in range(1, 3):\n",
        "    display(Image(filename=f'dqn_test_gifs/test_ep_{i}.gif'))\n",
        "print(\"Displaying Dyna-Q Test GIFs\")\n",
        "for i in range(1, 3):\n",
        "    display(Image(filename=f'dynaq_test_gifs/test_ep_{i}.gif'))"
      ],
      "metadata": {
        "id": "DddCRd-LGwYi"
      },
      "id": "DddCRd-LGwYi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# During training\n",
        "rewards = []\n",
        "avg_rewards = []\n",
        "num_steps = []\n",
        "epsilons = []\n",
        "\n",
        "# After training\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(rewards, label=\"Total Reward\")\n",
        "plt.plot(avg_rewards, label=\"Avg Reward (Last 100)\")\n",
        "plt.title(\"Training Rewards\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(num_steps, label=\"Steps per Episode\")\n",
        "plt.title(\"Steps per Episode\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Steps\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.plot(epsilons, label=\"Epsilon\")\n",
        "plt.title(\"Epsilon Decay\")\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Epsilon\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jCK4hS01Hg_G"
      },
      "id": "jCK4hS01Hg_G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "from IPython.display import Image\n",
        "imageio.mimsave('dqn_test.gif', frames)  # 'frames' from render()\n",
        "Image(filename='dqn_test.gif')"
      ],
      "metadata": {
        "id": "Ztrc3vEvHh38"
      },
      "id": "Ztrc3vEvHh38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Report\n",
        "\n",
        "### Purpose\n",
        "This project compares the performance of DQN and Dyna-Q in solving a 6x6 maze, evaluating their ability to learn an optimal path from start to goal.\n",
        "\n",
        "### Methods\n",
        "- **Environment:** A 6x6 maze with walls, a start at (0,0), and a goal at (5,5).\n",
        "- **Agents:**\n",
        "  - **DQN:** Uses a neural network with experience replay and a target network for stable Q-value estimation.\n",
        "  - **Dyna-Q:** Uses a Q-table with model-based planning to simulate additional experiences.\n",
        "- **Training:** 1000 episodes with a reward of +1 for reaching the goal and -0.01 per step.\n",
        "- **Testing:** 10 episodes to measure success rate and average steps to the goal.\n",
        "\n",
        "### Results\n",
        "*(Run your notebook to fill in these values)*\n",
        "- **DQN:**\n",
        "  - Success Rate: XX%\n",
        "  - Avg Steps: YY\n",
        "- **Dyna-Q:**\n",
        "  - Success Rate: ZZ%\n",
        "  - Avg Steps: WW\n",
        "\n",
        "### Conclusion\n",
        "Both agents learned to navigate the maze, with [DQN/Dyna-Q] outperforming the other in [success rate/avg steps]. Dyna-Q’s planning may improve efficiency in small environments, while DQN’s neural network scales better to complex tasks."
      ],
      "metadata": {
        "id": "EoGpX9HHHsN0"
      },
      "id": "EoGpX9HHHsN0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}